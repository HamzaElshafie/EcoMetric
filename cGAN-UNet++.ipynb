{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c43c56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from __future__ import print_function, division\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5cc74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dbc29bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=1, is_val = False):\n",
    "       \n",
    "        path_SAR = glob('Datasets/v_2/agri/s1/*')\n",
    "        \n",
    "        batch_images = np.random.choice(path_SAR, size=batch_size)\n",
    "        img_res=(128,128)\n",
    "        imgs_A = []\n",
    "        imgs_B = []\n",
    "        for img_path in batch_images:\n",
    "            img_B = imread(img_path)\n",
    "            img_A = imread(img_path.replace('/s1', '/s2').replace(\"_s1_\",\"_s2_\"))\n",
    "    \n",
    "            # decreasing the resolution \n",
    "            img_A = transform.resize(img_A, img_res)  #Ground Truth image\n",
    "            img_B = transform.resize(img_B, img_res)  #Input image\n",
    "\n",
    "            # If training => do random flip , this is a trick to avoid overfitting \n",
    "            if not is_val and np.random.random() < 0.5:\n",
    "                img_A = np.fliplr(img_A)\n",
    "                img_B = np.fliplr(img_B)\n",
    "\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "            \n",
    "        \n",
    "        imgs_A = np.array(imgs_A)/127.5 - 1.  #normalizing the images\n",
    "        imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "        return imgs_A, imgs_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8292c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(batch_size=1, is_val=False):\n",
    "        \n",
    "        path_SAR = glob('Datasets/v_2/agri/s1/*')\n",
    "        \n",
    "        n_batches=batch_size\n",
    "        img_res=(128,128)\n",
    "        for i in range(n_batches-1):\n",
    "            batch = path_SAR[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img in batch:\n",
    "                img_B = imread(img)\n",
    "                img_A = imread(img.replace('/s1', '/s2').replace(\"_s1_\",\"_s2_\"))\n",
    "                \n",
    "                img_A = transform.resize(img_A, img_res)#Ground truth image\n",
    "                img_B = transform.resize(img_B, img_res)# input image\n",
    "                \n",
    " # when training => do random flip , this is a trick to avoid overfitting \n",
    "                if not is_val and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "                \n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "            # normalizing the images \n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "def imread(path):\n",
    "    \n",
    "    return np.array(Image.open(path).convert('RGB')).astype(np.float32)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b77f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# path_SAR = glob('Datasets/v_2/agri/s1/*')\n",
    "\n",
    "\n",
    "# # Open an image file\n",
    "# image = Image.open(path_SAR[1].replace('/s1', '/s2').replace(\"_s1_\",\"_s2_\"))\n",
    "\n",
    "# # Display the image\n",
    "# image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9674657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "        \"\"\"U-Net++ Generator\"\"\"\n",
    "        \n",
    "#         nb_filter = [32,64,128,256,512]\n",
    "        nb_filter=[8, 16, 32, 64, 128]\n",
    "        inputs = Input(shape=img_shape)\n",
    "        \n",
    "        conv1_1 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name='problem_node') (inputs)\n",
    "        conv1_1 = Dropout(0.5) (conv1_1)\n",
    "        conv1_1 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_1)\n",
    "        conv1_1 = Dropout(0.5) (conv1_1)\n",
    "        pool1_1 = MaxPooling2D((2, 2), strides=(2, 2)) (conv1_1)\n",
    "        \n",
    "        conv2_1 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool1_1)\n",
    "        conv2_1 = Dropout(0.5) (conv2_1)\n",
    "        conv2_1 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2_1)\n",
    "        conv2_1 = Dropout(0.5) (conv2_1)\n",
    "        pool2_1 = MaxPooling2D((2, 2), strides=(2, 2)) (conv2_1)\n",
    "        \n",
    "        up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
    "        conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=3)\n",
    "        conv1_2 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_2)\n",
    "        conv1_2 = Dropout(0.5) (conv1_2)\n",
    "        conv1_2 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_2)\n",
    "        conv1_2 = Dropout(0.5) (conv1_2)\n",
    "        \n",
    "        conv3_1 = Conv2D(nb_filter[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool2_1)\n",
    "        conv3_1 = Dropout(0.5) (conv3_1)\n",
    "        conv3_1 = Conv2D(nb_filter[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3_1)\n",
    "        conv3_1 = Dropout(0.5) (conv3_1)\n",
    "        pool3_1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3_1')(conv3_1)\n",
    "\n",
    "        up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
    "        conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=3) #x10\n",
    "        conv2_2 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2_2)\n",
    "        conv2_2 = Dropout(0.5) (conv2_2)\n",
    "        conv2_2 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2_2)\n",
    "        conv2_2 = Dropout(0.5) (conv2_2)\n",
    "\n",
    "        up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
    "        conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=3)\n",
    "        conv1_3 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_3)\n",
    "        conv1_3 = Dropout(0.5) (conv1_3)\n",
    "        conv1_3 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_3)\n",
    "        conv1_3 = Dropout(0.5) (conv1_3)\n",
    "\n",
    "        conv4_1 = Conv2D(nb_filter[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool3_1)\n",
    "        conv4_1 = Dropout(0.5) (conv4_1)\n",
    "        conv4_1 = Conv2D(nb_filter[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv4_1)\n",
    "        conv4_1 = Dropout(0.5) (conv4_1)\n",
    "        pool4_1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4_1')(conv4_1)\n",
    "\n",
    "        up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
    "        conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=3) #x20\n",
    "        conv3_2 = Conv2D(nb_filter[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3_2)\n",
    "        conv3_2 = Dropout(0.5) (conv3_2)\n",
    "        conv3_2 = Conv2D(nb_filter[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3_2)\n",
    "        conv3_2 = Dropout(0.5) (conv3_2)\n",
    "\n",
    "        up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
    "        conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=3)\n",
    "        conv2_3 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2_3)\n",
    "        conv2_3 = Dropout(0.5) (conv2_3)\n",
    "        conv2_3 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2_3)\n",
    "        conv2_3 = Dropout(0.5) (conv2_3)\n",
    "\n",
    "        up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
    "        conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=3)\n",
    "        conv1_4 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_4)\n",
    "        conv1_4 = Dropout(0.5) (conv1_4)\n",
    "        conv1_4 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_4)\n",
    "        conv1_4 = Dropout(0.5) (conv1_4)\n",
    "\n",
    "        conv5_1 = Conv2D(nb_filter[4], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool4_1)\n",
    "        conv5_1 = Dropout(0.5) (conv5_1)\n",
    "        conv5_1 = Conv2D(nb_filter[4], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv5_1)\n",
    "        conv5_1 = Dropout(0.5) (conv5_1)\n",
    "\n",
    "        up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
    "        conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=3) #x30\n",
    "        conv4_2 = Conv2D(nb_filter[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv4_2)\n",
    "        conv4_2 = Dropout(0.5) (conv4_2)\n",
    "        conv4_2 = Conv2D(nb_filter[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv4_2)\n",
    "        conv4_2 = Dropout(0.5) (conv4_2)\n",
    "\n",
    "        up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
    "        conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=3)\n",
    "        conv3_3 = Conv2D(nb_filter[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3_3)\n",
    "        conv3_3 = Dropout(0.5) (conv3_3)\n",
    "        conv3_3 = Conv2D(nb_filter[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3_3)\n",
    "        conv3_3 = Dropout(0.5) (conv3_3)\n",
    "\n",
    "        up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
    "        conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=3)\n",
    "        conv2_4 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2_4)\n",
    "        conv2_4 = Dropout(0.5) (conv2_4)\n",
    "        conv2_4 = Conv2D(nb_filter[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2_4)\n",
    "        conv2_4 = Dropout(0.5) (conv2_4)\n",
    "\n",
    "        up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
    "        conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=3)\n",
    "        conv1_5 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_5)\n",
    "        conv1_5 = Dropout(0.5) (conv1_5)\n",
    "        conv1_5 = Conv2D(nb_filter[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1_5)\n",
    "        conv1_5 = Dropout(0.5) (conv1_5)\n",
    "        \n",
    "#         u7 = UpSampling2D(size=2)(conv1_5) # upsamples to twice the size\n",
    "        output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(conv1_5)\n",
    "        \n",
    "#         nestnet_output_4 = Conv2D(1, (1, 1), activation='sigmoid', kernel_initializer = 'he_normal',  name='output_4', padding='same')(conv1_5)\n",
    "\n",
    "        return Model([inputs], [output_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bdbdfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "        # a small function to make one layer of the discriminator\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=img_shape)\n",
    "        img_B = Input(shape=img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, df, bn=False)\n",
    "        d2 = d_layer(d1, df*2)\n",
    "#         d3 = d_layer(d2, df*4)\n",
    "#         d4 = d_layer(d3, df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d2)\n",
    "\n",
    "        return Model([img_A, img_B], validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c36107f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**2)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = generator(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([fake_A, img_B])\n",
    "\n",
    "def ssim_loss(valid, fake_A):\n",
    "    return 1 - tf.image.ssim(valid, fake_A, 1.0)\n",
    "    \n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "\n",
    "# combined.compile(loss=['mse', 'mae'],\n",
    "#                               loss_weights=[1, 100],\n",
    "#                               optimizer=optimizer)\n",
    "\n",
    "combined.compile(loss=['mse', ssim_loss], loss_weights=[50, 50], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "638cedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images( epoch, batch_i):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "\n",
    "        imgs_A, imgs_B = load_data(batch_size=3)\n",
    "        fake_A = generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Output', 'Ground Truth']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35c7c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epochs, batch_size=1, show_interval=10):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + disc_patch)\n",
    "        fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(batch_size)):\n",
    "\n",
    "                \n",
    "                #  Train Discriminator\n",
    "                \n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "               \n",
    "                #  Train Generator\n",
    "                g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch%10==0:\n",
    "                  print (\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        \n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "            # If at show interval => show generated image samples\n",
    "            if epoch % show_interval == 0:\n",
    "                    show_images(epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b82518f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 799ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(tf\u001b[38;5;241m.\u001b[39mDeviceSpec(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[72], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, batch_size, show_interval)\u001b[0m\n\u001b[0;32m      7\u001b[0m fake \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size,) \u001b[38;5;241m+\u001b[39m disc_patch)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_i, (imgs_A, imgs_B) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(load_batch(batch_size)):\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m         \n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m#  Train Discriminator\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# Condition on B and generate a translated version\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         fake_A \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(imgs_B)\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# Train the discriminators (original images = real / generated = Fake)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[65], line 12\u001b[0m, in \u001b[0;36mload_batch\u001b[1;34m(batch_size, is_val)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m     11\u001b[0m     img_B \u001b[38;5;241m=\u001b[39m imread(img)\n\u001b[1;32m---> 12\u001b[0m     img_A \u001b[38;5;241m=\u001b[39m \u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/s1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/s2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_s1_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_s2_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     img_A \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mresize(img_A, img_res)\u001b[38;5;66;03m#Ground truth image\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     img_B \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mresize(img_B, img_res)\u001b[38;5;66;03m# input image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[65], line 32\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(path):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:933\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    887\u001b[0m ):\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=0)):\n",
    "    train(epochs=1000, batch_size=10, show_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146d58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a16cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pkl', 'wb') as file:\n",
    "        pickle.dump(generator, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44e132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665dcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pkl', 'rb') as file:\n",
    "        generator1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db237a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_1( epoch, batch_i, generator1):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "\n",
    "        imgs_A, imgs_B = load_data(batch_size=3)\n",
    "        fake_A = generator1.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Output', 'Ground Truth']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_1(1,1,1,generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf1036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
